{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes made by kts4 / noami2\n",
    "- `glove` was not available and so needed to install `mittens` instead\n",
    "  - `from mittens import GloVe as glove`\n",
    "- Removed `merge` from import from `keras.layers`\n",
    "  - No longer available.\n",
    "  - Used `Concatenate()([x, text_embeddings])` instead of `merge([x, text_embeddings], mode='concat', concat_axis=1)`\n",
    "  - Code was available already, just commented out.\n",
    "- `set_session`, `clear_session` and `get_session` are no longer available from `keras.backend.tensorflow_backend`\n",
    "  - Loaded from `tf.compat.v1.keras.backend` and `tf.keras.backend` instead\n",
    "- Imported `Path`\n",
    "  - Allows us to ensure the required directories for saving are available\n",
    "- We do not have the `FastText` model and so all references to it and combined model have to be commented out\n",
    "- `tf.contrib.layers.l2_regularizer` no longer available\n",
    "  - Used `tf.keras.regularizers.l2` instead\n",
    "- `tf.contrib.layers.xavier_initializer` no longer available\n",
    "  - Used `tf.keras.initializers.GlorotUniform` instead\n",
    "- Changed `x_train_ner = np.asarray(x_train_dict_sorted.values())` to `x_train_ner = np.array(list(x_train_dict_sorted.values()))`\n",
    "  - Change due to Python 3\n",
    "- Changes to newlines / spacings / printing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.compat.v1.keras.backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_pickle(\"data/new_x_train.pkl\")\n",
    "x_dev   = pd.read_pickle(\"data/new_x_dev.pkl\")\n",
    "x_test  = pd.read_pickle(\"data/new_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/new_y_train.pkl\")\n",
    "y_dev   = pd.read_pickle(\"data/new_y_dev.pkl\")\n",
    "y_test  = pd.read_pickle(\"data/new_y_test.pkl\")\n",
    "\n",
    "\n",
    "ner_word2vec     = pd.read_pickle(\"data/new_ner_word2vec_limited_dict.pkl\")\n",
    "ner_fasttext     = pd.read_pickle(\"data/new_ner_fasttext_limited_dict.pkl\")\n",
    "ner_concat       = pd.read_pickle(\"data/new_ner_combined_limited_dict.pkl\")\n",
    "# ner_clinicalBERT = pd.read_pickle(\"data/new_ner_clinicalbert_limited_dict.pkl\")\n",
    "# ner_blueBERT      = pd.read_pickle(\"data/new_ner_bluebert_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/new_train_ids.pkl\")\n",
    "dev_ids   = pd.read_pickle(\"data/new_dev_ids.pkl\")\n",
    "test_ids  = pd.read_pickle(\"data/new_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prediction_cnn(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_cnn(predictions, probs, ground_truth, \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,\n",
    "                          sequence_name):\n",
    "    \n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict          = {}    \n",
    "    result_dict['auc']   = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc']   = acc\n",
    "    result_dict['F1']    = F1\n",
    "\n",
    "    result_path = \"results/09-cnn/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-new-cnn-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "    \n",
    "def print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n",
    "    \n",
    "def get_subvector_data(size, embed_name, data):\n",
    "    if embed_name == \"concat\":\n",
    "        vector_size = 200\n",
    "    elif embed_name == \"clinicalBERT\":\n",
    "        vector_size = 768\n",
    "    elif embed_name == \"blueBERT\":\n",
    "        vector_size = 768\n",
    "    else:\n",
    "        vector_size = 100\n",
    "\n",
    "    x_data = {}\n",
    "    for k, v in data.items():\n",
    "        number_of_additional_vector = len(v) - size\n",
    "        vector = []\n",
    "        for i in v:\n",
    "            vector.append(i)\n",
    "            \n",
    "        if number_of_additional_vector < 0: \n",
    "            number_of_additional_vector = np.abs(number_of_additional_vector)\n",
    "\n",
    "            temp = vector[:size]\n",
    "            for i in range(0, number_of_additional_vector):\n",
    "                temp.append(np.zeros(vector_size))\n",
    "            x_data[k] = np.asarray(temp)\n",
    "        else:\n",
    "            x_data[k] = np.asarray(vector[:size])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    elif embedding_name == \"clinicalBERT\":\n",
    "        input_dimension = 768\n",
    "    elif embedding_name == \"blueBERT\":\n",
    "        input_dimension = 768\n",
    "    else:\n",
    "        input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_img = Input(shape=(ner_limit, input_dimension), \n",
    "                      name = \"cnn_input\")\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4]\n",
    "\n",
    "    text_conv1d = Conv1D(filters=num_filter, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                        )(input_img)\n",
    "    text_conv1d = Dropout(0.2)(text_conv1d)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*2, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                        )(text_conv1d)   \n",
    "    text_conv1d = Dropout(0.2)(text_conv1d)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*3, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                        )(text_conv1d)   \n",
    "    text_conv1d = Dropout(0.2)(text_conv1d)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*4, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                        )(text_conv1d)   \n",
    "    text_conv1d = Dropout(0.2)(text_conv1d)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*5, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                        )(text_conv1d)   \n",
    "    text_conv1d = Dropout(0.2)(text_conv1d) \n",
    "\n",
    "    \n",
    "    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n",
    "    \n",
    "    x = GRU(number_of_unit)(sequence_input)\n",
    "    \n",
    "    concatenated = Concatenate()([x, text_embeddings])\n",
    "\n",
    "    concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    concatenated = Dropout(0.2)(concatenated)\n",
    "    \n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.01)\n",
    "    preds = Dense(1, \n",
    "                  activation='sigmoid',\n",
    "                  use_bias=False,\n",
    "                  kernel_initializer=tf.keras.initializers.GlorotUniform(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "                 )(concatenated)\n",
    "    \n",
    "    opt = Adam(lr=1e-3, decay = 0.01)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, input_img], \n",
    "                  outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8814710476703476 AUPRC:  0.5739049882319722 F1:  0.46731571627260077\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8882083904461392 AUPRC:  0.515831045422583 F1:  0.4621676891615542\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7069631961356277 AUPRC:  0.6450519319625181 F1:  0.5662828461765574\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7333899348565417 AUPRC:  0.23693684792541678 F1:  0.04289544235924933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 1/10 [1:06:25<9:57:49, 3985.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  2\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.880666846215998 AUPRC:  0.578469975736881 F1:  0.4790257104194858\n",
      "Embedding:  word2vec\n",
      "Iteration number:  2\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8874463752916969 AUPRC:  0.5198690362455033 F1:  0.47887323943661975\n",
      "Embedding:  word2vec\n",
      "Iteration number:  2\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7035289326143388 AUPRC:  0.639218362395086 F1:  0.558649289099526\n",
      "Embedding:  word2vec\n",
      "Iteration number:  2\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7448974174951792 AUPRC:  0.22921460920275546 F1:  0.05305039787798409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▍                                                             | 2/10 [2:05:37<8:17:22, 3730.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  3\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8812512792889847 AUPRC:  0.5734118760876075 F1:  0.4811827956989248\n",
      "Embedding:  word2vec\n",
      "Iteration number:  3\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8822594592605539 AUPRC:  0.5100598238416777 F1:  0.4665314401622718\n",
      "Embedding:  word2vec\n",
      "Iteration number:  3\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7057624792597745 AUPRC:  0.6466208894978152 F1:  0.5583756345177664\n",
      "Embedding:  word2vec\n",
      "Iteration number:  3\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7331855213165748 AUPRC:  0.2267587684997862 F1:  0.0481283422459893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████                                                      | 3/10 [3:05:50<7:08:59, 3677.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  4\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8809841098841906 AUPRC:  0.5772053325048957 F1:  0.49066666666666664\n",
      "Embedding:  word2vec\n",
      "Iteration number:  4\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8854769417587371 AUPRC:  0.5109680644867606 F1:  0.45703125\n",
      "Embedding:  word2vec\n",
      "Iteration number:  4\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7040440930500174 AUPRC:  0.6425838381348351 F1:  0.5475469412477286\n",
      "Embedding:  word2vec\n",
      "Iteration number:  4\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7319852381021323 AUPRC:  0.22776222129735008 F1:  0.05804749340369393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████▊                                              | 4/10 [3:41:50<5:07:47, 3077.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  5\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8801115001346621 AUPRC:  0.5752620333762176 F1:  0.4712328767123288\n",
      "Embedding:  word2vec\n",
      "Iteration number:  5\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.889382844386386 AUPRC:  0.5156510854855064 F1:  0.466403162055336\n",
      "Embedding:  word2vec\n",
      "Iteration number:  5\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7029813655765782 AUPRC:  0.6399748402918701 F1:  0.554958183990442\n",
      "Embedding:  word2vec\n",
      "Iteration number:  5\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7304795410657495 AUPRC:  0.2210615929813683 F1:  0.0374331550802139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████▌                                      | 5/10 [4:05:31<3:26:43, 2480.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  6\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8799644492324266 AUPRC:  0.5700489639817498 F1:  0.4772117962466488\n",
      "Embedding:  word2vec\n",
      "Iteration number:  6\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.890691455924246 AUPRC:  0.5246972853653601 F1:  0.46799999999999997\n",
      "Embedding:  word2vec\n",
      "Iteration number:  6\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7040192131426124 AUPRC:  0.6401833206397936 F1:  0.5583482944344704\n",
      "Embedding:  word2vec\n",
      "Iteration number:  6\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.736796022863982 AUPRC:  0.2277745161308613 F1:  0.03763440860215054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▏                              | 6/10 [5:45:53<4:05:38, 3684.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  7\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8811494748182063 AUPRC:  0.5795812993400264 F1:  0.4925975773889637\n",
      "Embedding:  word2vec\n",
      "Iteration number:  7\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8862213247717787 AUPRC:  0.5237477329395441 F1:  0.4745098039215687\n",
      "Embedding:  word2vec\n",
      "Iteration number:  7\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7039663172050205 AUPRC:  0.6433534680001458 F1:  0.554325052379527\n",
      "Embedding:  word2vec\n",
      "Iteration number:  7\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7351414107360887 AUPRC:  0.22294348373237438 F1:  0.05774278215223097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████▉                       | 7/10 [6:07:34<2:25:15, 2905.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  8\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8801077295987072 AUPRC:  0.5740541626841427 F1:  0.4920212765957447\n",
      "Embedding:  word2vec\n",
      "Iteration number:  8\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8879086440423294 AUPRC:  0.5076882691125678 F1:  0.47036328871892924\n",
      "Embedding:  word2vec\n",
      "Iteration number:  8\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7036933699855488 AUPRC:  0.6437000447304045 F1:  0.5561209218796768\n",
      "Embedding:  word2vec\n",
      "Iteration number:  8\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7274674575712639 AUPRC:  0.22217135917760122 F1:  0.021798365122615803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████▌               | 8/10 [6:33:35<1:22:34, 2477.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  9\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8817242122273095 AUPRC:  0.5807161875004496 F1:  0.4922680412371134\n",
      "Embedding:  word2vec\n",
      "Iteration number:  9\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.8861507962061764 AUPRC:  0.5142343388948157 F1:  0.4466800804828973\n",
      "Embedding:  word2vec\n",
      "Iteration number:  9\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.704966322222817 AUPRC:  0.6441629669972803 F1:  0.5543672014260249\n",
      "Embedding:  word2vec\n",
      "Iteration number:  9\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7395923173601215 AUPRC:  0.23837772207491442 F1:  0.03814713896457765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████        | 9/10 [6:59:42<36:32, 2192.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  10\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.882200915701589 AUPRC:  0.5821164319643676 F1:  0.48648648648648646\n",
      "Embedding:  word2vec\n",
      "Iteration number:  10\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.887420310387018 AUPRC:  0.5128183167656285 F1:  0.465979381443299\n",
      "Embedding:  word2vec\n",
      "Iteration number:  10\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7037415617389674 AUPRC:  0.6391253925756186 F1:  0.5591461606878151\n",
      "Embedding:  word2vec\n",
      "Iteration number:  10\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "AUC:  0.7367146711010273 AUPRC:  0.2263525131126095 F1:  0.03763440860215054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 10/10 [7:33:30<00:00, 2721.01s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6541",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embed_dict, embed_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embedding_dict, embedding_types):  \n\u001b[1;32m---> 21\u001b[0m     temp_train_ner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     temp_dev_ner   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((k, embed_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dev_ids)\n\u001b[0;32m     23\u001b[0m     temp_test_ner  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((k, embed_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m test_ids)\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embed_dict, embed_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embedding_dict, embedding_types):  \n\u001b[1;32m---> 21\u001b[0m     temp_train_ner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((k, \u001b[43membed_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m train_ids)\n\u001b[0;32m     22\u001b[0m     temp_dev_ner   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((k, embed_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dev_ids)\n\u001b[0;32m     23\u001b[0m     temp_test_ner  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((k, embed_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m test_ids)\n",
      "\u001b[1;31mKeyError\u001b[0m: 6541"
     ]
    }
   ],
   "source": [
    "embedding_types = ['word2vec', 'fasttext', 'concat']#['word2vec', 'fasttext', 'concat', 'clinicalBERT', 'blueBERT']\n",
    "embedding_dict  = [ner_word2vec, ner_fasttext, ner_concat]#[ner_word2vec, ner_fasttext, ner_concat, ner_clinicalBERT, ner_blueBERT]\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "num_epoch        = 100\n",
    "model_patience   = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size       = 64\n",
    "\n",
    "filter_number    = 32\n",
    "ner_representation_limit = 64\n",
    "activation_func  = \"relu\"\n",
    "\n",
    "sequence_model   = \"GRU\"\n",
    "sequence_hidden_unit = 256\n",
    "\n",
    "maxiter = 11\n",
    "for embed_dict, embed_name in zip(embedding_dict, embedding_types):  \n",
    "    \n",
    "    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n",
    "    temp_dev_ner   = dict((k, embed_dict[k]) for k in dev_ids)\n",
    "    temp_test_ner  = dict((k, embed_dict[k]) for k in test_ids)\n",
    "\n",
    "    x_train_dict = {}\n",
    "    x_dev_dict   = {}\n",
    "    x_test_dict  = {}\n",
    "\n",
    "    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n",
    "    x_dev_dict   = get_subvector_data(ner_representation_limit, embed_name, temp_dev_ner)\n",
    "    x_test_dict  = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n",
    "\n",
    "    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n",
    "    \n",
    "    x_dev_dict_sorted   = collections.OrderedDict(sorted(x_dev_dict.items()))\n",
    "    x_test_dict_sorted  = collections.OrderedDict(sorted(x_test_dict.items()))\n",
    "\n",
    "    x_train_ner = np.asarray(list(x_train_dict_sorted.values()))\n",
    "    x_dev_ner   = np.asarray(list(x_dev_dict_sorted.values()))\n",
    "    x_test_ner  = np.asarray(list(x_test_dict_sorted.values()))\n",
    "        \n",
    "    for iteration in tqdm(range(1,maxiter)):\n",
    "        for each_problem in target_problems:  \n",
    "            \n",
    "            print (\"Embedding: \", embed_name)\n",
    "            print (\"Iteration number: \", iteration)\n",
    "            print (\"Problem type: \", each_problem)\n",
    "            print (\"__________________\")\n",
    "            \n",
    "            \n",
    "            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, \n",
    "                                                   patience=model_patience)\n",
    "            \n",
    "            best_model_name = \"results/Best Models/\" + str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(best_model_name, \n",
    "                                         monitor=monitor_criteria, \n",
    "                                         verbose=0,\n",
    "                                         save_best_only=True, \n",
    "                                         mode='min')\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, \n",
    "                                          factor=0.2,\n",
    "                                          patience=2, \n",
    "                                          min_lr=0.00001, \n",
    "                                          epsilon=1e-4, \n",
    "                                          mode='min')\n",
    "            \n",
    "\n",
    "            callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n",
    "            \n",
    "            model = proposedmodel(sequence_model, \n",
    "                                  sequence_hidden_unit, \n",
    "                                  embed_name, \n",
    "                                  ner_representation_limit,\n",
    "                                  filter_number)\n",
    "            \n",
    "            model.fit([x_train, x_train_ner], \n",
    "                      y_train[each_problem], \n",
    "                      epochs=num_epoch, \n",
    "                      verbose=0, \n",
    "                      validation_data=([x_dev, x_dev_ner], y_dev[each_problem]), \n",
    "                      callbacks=callbacks, \n",
    "                      batch_size=batch_size)\n",
    "            \n",
    "            \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test, x_test_ner])\n",
    "            print_scores_cnn(predictions, \n",
    "                             probs, \n",
    "                             y_test[each_problem], \n",
    "                             embed_name, \n",
    "                             each_problem, \n",
    "                             iteration, \n",
    "                             sequence_hidden_unit)\n",
    "            \n",
    "            model.load_weights(best_model_name)\n",
    "                      \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test, x_test_ner])\n",
    "            save_scores_cnn(predictions, \n",
    "                            probs, \n",
    "                            y_test[each_problem], \n",
    "                            embed_name, \n",
    "                            each_problem, \n",
    "                            iteration,\n",
    "                            sequence_hidden_unit, \n",
    "                            sequence_model \n",
    "                            )\n",
    "            del model\n",
    "            clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
