{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:30:46.725497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "# from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "# from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm   = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm  = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev   = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test  = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "# ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "# ner_concat   = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids   = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids  = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_cnn(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_cnn(predictions, probs, ground_truth, \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,\n",
    "                          sequence_name, type_of_ner):\n",
    "    \n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "\n",
    "    result_path = \"results/cnn/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-cnn-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n",
    "    \n",
    "def get_subvector_data(size, embed_name, data):\n",
    "    if embed_name == \"concat\":\n",
    "        vector_size = 200\n",
    "    else:\n",
    "        vector_size = 100\n",
    "\n",
    "    x_data = {}\n",
    "    for k, v in data.items():\n",
    "        number_of_additional_vector = len(v) - size\n",
    "        vector = []\n",
    "        for i in v:\n",
    "            vector.append(i)\n",
    "            \n",
    "        if number_of_additional_vector < 0: \n",
    "            number_of_additional_vector = np.abs(number_of_additional_vector)\n",
    "\n",
    "            temp = vector[:size]\n",
    "            for i in range(0, number_of_additional_vector):\n",
    "                temp.append(np.zeros(vector_size))\n",
    "            x_data[k] = np.asarray(temp)\n",
    "        else:\n",
    "            x_data[k] = np.asarray(vector[:size])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    else:\n",
    "        input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_img = Input(shape=(ner_limit, input_dimension), \n",
    "                      name = \"cnn_input\")\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4]\n",
    "\n",
    "\n",
    "    text_conv1d = Conv1D(filters=num_filter, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(input_img)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*2, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(text_conv1d)   \n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*3, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(text_conv1d)   \n",
    "\n",
    "    \n",
    "    #concat_conv = keras.layers.Concatenate()([text_conv1d, text_conv1d_2, text_conv1d_3])\n",
    "    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n",
    "    #text_embeddings = Dense(128, activation=\"relu\")(text_embeddings)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    concatenated = Concatenate()([x, text_embeddings])\n",
    "#     concatenated = merge([x, text_embeddings], mode='concat', concat_axis=1)\n",
    "\n",
    "    concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    concatenated = Dropout(0.2)(concatenated)\n",
    "    #concatenated = Dense(256, activation='relu')(concatenated)\n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    \n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.01)\n",
    "    preds = Dense(1, \n",
    "                  activation='sigmoid',\n",
    "                  use_bias=False,\n",
    "                  kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "                 )(concatenated)\n",
    "    \n",
    "    \n",
    "    #opt = Adam(lr=1e-4, decay = 0.01)\n",
    "    opt = Adam(lr=1e-3, decay = 0.01)\n",
    "    #opt = Adam(lr=0.001)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, input_img], \n",
    "                  outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:30:55.093997: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/dense/kernel/m/Assign' id:828 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense/kernel/m, training/Adam/dense/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:31:06.512589: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:483 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:33:01.965768: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:417 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8844960948020468 AUPRC:  0.5860839967322752 F1:  0.47126436781609193\n",
      "0.8845214112577431 0.5861584727458088 0.9167420814479638 0.46974063400576366\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:33:05.220099: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/gru/gru_cell/recurrent_kernel/v/Assign' id:888 op device:{requested: '', assigned: ''} def:{{{node training/Adam/gru/gru_cell/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/gru/gru_cell/recurrent_kernel/v, training/Adam/gru/gru_cell/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:33:16.946246: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:483 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:35:06.439127: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:417 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8882198896687917 AUPRC:  0.5219384926343781 F1:  0.45378151260504196\n",
      "0.8883069004535293 0.5243659129999415 0.9425339366515837 0.44782608695652176\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:35:09.771369: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv1d/kernel/v/Assign' id:845 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv1d/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv1d/kernel/v, training/Adam/conv1d/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:35:21.612221: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:483 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:37:01.152650: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:417 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.6933934019321862 AUPRC:  0.6371932210040905 F1:  0.561209653969177\n",
      "0.6974170474616641 0.6399887353229705 0.6595022624434389 0.5438011518642013\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:37:04.593601: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv1d_1/kernel/m/Assign' id:783 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv1d_1/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv1d_1/kernel/m, training/Adam/conv1d_1/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:37:16.845877: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:483 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:38:50.617867: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:417 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7374757926798581 AUPRC:  0.22339617299715325 F1:  0.04851752021563343\n",
      "0.7289131749180104 0.21185260471650025 0.9192307692307692 0.0\n",
      "Embedding:  word2vec\n",
      "Iteration number:  2\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:38:54.618336: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/conv1d_2/kernel/v/Assign' id:869 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv1d_2/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv1d_2/kernel/v, training/Adam/conv1d_2/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-05 21:39:09.723337: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:483 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Path(\"/results/cnn/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "embedding_types = ['word2vec']#, 'fasttext', 'concat']\n",
    "embedding_dict = [ner_word2vec]#, ner_fasttext, ner_concat]\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "#monitor_criteria = 'val_acc'\n",
    "batch_size = 64\n",
    "\n",
    "filter_number = 32\n",
    "ner_representation_limit = 64\n",
    "activation_func = \"relu\"\n",
    "\n",
    "sequence_model = \"GRU\"\n",
    "sequence_hidden_unit = 256\n",
    "\n",
    "maxiter = 11\n",
    "for embed_dict, embed_name in zip(embedding_dict, embedding_types):  \n",
    "    \n",
    "    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n",
    "    tem_dev_ner    = dict((k, embed_dict[k]) for k in dev_ids)\n",
    "    temp_test_ner  = dict((k, embed_dict[k]) for k in test_ids)\n",
    "\n",
    "    x_train_dict = {}\n",
    "    x_dev_dict   = {}\n",
    "    x_test_dict  = {}\n",
    "\n",
    "    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n",
    "    x_dev_dict   = get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n",
    "    x_test_dict  = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n",
    "\n",
    "    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n",
    "    \n",
    "    x_dev_dict_sorted   = collections.OrderedDict(sorted(x_dev_dict.items()))\n",
    "    x_test_dict_sorted  = collections.OrderedDict(sorted(x_test_dict.items()))\n",
    "\n",
    "    x_train_ner = np.array(list(x_train_dict_sorted.values()))\n",
    "    x_dev_ner   = np.array(list(x_dev_dict_sorted.values()))\n",
    "    x_test_ner  = np.array(list(x_test_dict_sorted.values()))\n",
    "        \n",
    "    for iteration in range(1,maxiter):\n",
    "        for each_problem in target_problems:  \n",
    "            \n",
    "            print (\"Embedding: \", embed_name)\n",
    "            print (\"Iteration number: \", iteration)\n",
    "            print (\"Problem type: \", each_problem)\n",
    "            print (\"__________________\")\n",
    "            \n",
    "            \n",
    "            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, \n",
    "                                                   patience=model_patience)\n",
    "            \n",
    "            best_model_name = str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(best_model_name, \n",
    "                                         monitor=monitor_criteria, \n",
    "                                         verbose=0,\n",
    "                                         save_best_only=True, \n",
    "                                         mode='min')\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, \n",
    "                                          factor=0.2,\n",
    "                                          patience=2, \n",
    "                                          min_lr=0.00001, \n",
    "                                          epsilon=1e-4, \n",
    "                                          mode='min')\n",
    "            \n",
    "\n",
    "            callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n",
    "            \n",
    "            #model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\n",
    "            model = proposedmodel(sequence_model, \n",
    "                                  sequence_hidden_unit, \n",
    "                                  embed_name, \n",
    "                                  ner_representation_limit,\n",
    "                                  filter_number)\n",
    "            \n",
    "            model.fit([x_train_lstm, x_train_ner], \n",
    "                      y_train[each_problem], \n",
    "                      epochs=num_epoch, \n",
    "                      verbose=0, \n",
    "                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), \n",
    "                      callbacks=callbacks, \n",
    "                      batch_size=batch_size)\n",
    "            \n",
    "            \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            print_scores_cnn(predictions, \n",
    "                             probs, \n",
    "                             y_test[each_problem], \n",
    "                             embed_name, \n",
    "                             each_problem, \n",
    "                             iteration, \n",
    "                             sequence_hidden_unit)\n",
    "            \n",
    "            model.load_weights(best_model_name)\n",
    "                      \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            save_scores_cnn(predictions, \n",
    "                            probs, \n",
    "                            y_test[each_problem], \n",
    "                            embed_name, \n",
    "                            each_problem, \n",
    "                            iteration,\n",
    "                            sequence_hidden_unit, \n",
    "                            sequence_model, \n",
    "                            type_of_ner)\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLH_project_py38] *",
   "language": "python",
   "name": "conda-env-DLH_project_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
