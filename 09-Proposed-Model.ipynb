{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 09:21:38.598447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "# from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "# from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm   = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm  = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev   = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test  = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "# ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "# ner_concat   = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids   = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids  = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_cnn(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_cnn(predictions, probs, ground_truth, \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,\n",
    "                          sequence_name, type_of_ner):\n",
    "    \n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "\n",
    "    result_path = \"results/cnn/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-cnn-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n",
    "    \n",
    "def get_subvector_data(size, embed_name, data):\n",
    "    if embed_name == \"concat\":\n",
    "        vector_size = 200\n",
    "    else:\n",
    "        vector_size = 100\n",
    "\n",
    "    x_data = {}\n",
    "    for k, v in data.items():\n",
    "        number_of_additional_vector = len(v) - size\n",
    "        vector = []\n",
    "        for i in v:\n",
    "            vector.append(i)\n",
    "            \n",
    "        if number_of_additional_vector < 0: \n",
    "            number_of_additional_vector = np.abs(number_of_additional_vector)\n",
    "\n",
    "            temp = vector[:size]\n",
    "            for i in range(0, number_of_additional_vector):\n",
    "                temp.append(np.zeros(vector_size))\n",
    "            x_data[k] = np.asarray(temp)\n",
    "        else:\n",
    "            x_data[k] = np.asarray(vector[:size])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    else:\n",
    "        input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_img = Input(shape=(ner_limit, input_dimension), \n",
    "                      name = \"cnn_input\")\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4]\n",
    "\n",
    "\n",
    "    text_conv1d = Conv1D(filters=num_filter, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(input_img)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*2, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(text_conv1d)   \n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*3, \n",
    "                         kernel_size=3, \n",
    "                         padding = 'valid', \n",
    "                         strides = 1, \n",
    "                         dilation_rate=1, \n",
    "                         activation='relu',\n",
    "                         kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal()\n",
    "                        )(text_conv1d)   \n",
    "\n",
    "    \n",
    "    #concat_conv = keras.layers.Concatenate()([text_conv1d, text_conv1d_2, text_conv1d_3])\n",
    "    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n",
    "    #text_embeddings = Dense(128, activation=\"relu\")(text_embeddings)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    concatenated = Concatenate()([x, text_embeddings])\n",
    "#     concatenated = merge([x, text_embeddings], mode='concat', concat_axis=1)\n",
    "\n",
    "    concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    concatenated = Dropout(0.2)(concatenated)\n",
    "    #concatenated = Dense(256, activation='relu')(concatenated)\n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    \n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.01)\n",
    "    preds = Dense(1, \n",
    "                  activation='sigmoid',\n",
    "                  use_bias=False,\n",
    "                  kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "                 )(concatenated)\n",
    "    \n",
    "    \n",
    "    #opt = Adam(lr=1e-4, decay = 0.01)\n",
    "    opt = Adam(lr=1e-3, decay = 0.01)\n",
    "    #opt = Adam(lr=0.001)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, input_img], \n",
    "                  outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [early_stopping_monitor, checkpoint, reduce_lr]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mproposedmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msequence_hidden_unit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                      \u001b[49m\u001b[43membed_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mner_representation_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfilter_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([x_train_lstm, x_train_ner], \n\u001b[1;32m     80\u001b[0m           y_train[each_problem], \n\u001b[1;32m     81\u001b[0m           epochs\u001b[38;5;241m=\u001b[39mnum_epoch, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \n\u001b[1;32m     85\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     88\u001b[0m probs, predictions \u001b[38;5;241m=\u001b[39m make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mproposedmodel\u001b[0;34m(layer_name, number_of_unit, embedding_name, ner_limit, num_filter)\u001b[0m\n\u001b[1;32m    118\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.2\u001b[39m)(concatenated)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#concatenated = Dense(256, activation='relu')(concatenated)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#concatenated = Dense(512, activation='relu')(concatenated)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#concatenated = Dense(512, activation='relu')(concatenated)\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m logits_regularizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrib\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39ml2_regularizer(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    124\u001b[0m preds \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m    125\u001b[0m               activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    126\u001b[0m               use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m               kernel_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mglorot_normal(), \n\u001b[1;32m    128\u001b[0m               kernel_regularizer\u001b[38;5;241m=\u001b[39mlogits_regularizer\n\u001b[1;32m    129\u001b[0m              )(concatenated)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#opt = Adam(lr=1e-4, decay = 0.01)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "embedding_types = ['word2vec']#, 'fasttext', 'concat']\n",
    "embedding_dict = [ner_word2vec]#, ner_fasttext, ner_concat]\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "#monitor_criteria = 'val_acc'\n",
    "batch_size = 64\n",
    "\n",
    "filter_number = 32\n",
    "ner_representation_limit = 64\n",
    "activation_func = \"relu\"\n",
    "\n",
    "sequence_model = \"GRU\"\n",
    "sequence_hidden_unit = 256\n",
    "\n",
    "maxiter = 11\n",
    "for embed_dict, embed_name in zip(embedding_dict, embedding_types):  \n",
    "    \n",
    "    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n",
    "    tem_dev_ner    = dict((k, embed_dict[k]) for k in dev_ids)\n",
    "    temp_test_ner  = dict((k, embed_dict[k]) for k in test_ids)\n",
    "\n",
    "    x_train_dict = {}\n",
    "    x_dev_dict   = {}\n",
    "    x_test_dict  = {}\n",
    "\n",
    "    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n",
    "    x_dev_dict   = get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n",
    "    x_test_dict  = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n",
    "\n",
    "    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n",
    "    x_dev_dict_sorted   = collections.OrderedDict(sorted(x_dev_dict.items()))\n",
    "    x_test_dict_sorted  = collections.OrderedDict(sorted(x_test_dict.items()))\n",
    "\n",
    "    x_train_ner = np.asarray(x_train_dict_sorted.values())\n",
    "    x_dev_ner   = np.asarray(x_dev_dict_sorted.values())\n",
    "    x_test_ner  = np.asarray(x_test_dict_sorted.values())\n",
    "        \n",
    "    for iteration in range(1,maxiter):\n",
    "        for each_problem in target_problems:  \n",
    "            \n",
    "            print (\"Embedding: \", embed_name)\n",
    "            print (\"Iteration number: \", iteration)\n",
    "            print (\"Problem type: \", each_problem)\n",
    "            print (\"__________________\")\n",
    "            \n",
    "            \n",
    "            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, \n",
    "                                                   patience=model_patience)\n",
    "            \n",
    "            best_model_name = str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(best_model_name, \n",
    "                                         monitor=monitor_criteria, \n",
    "                                         verbose=0,\n",
    "                                         save_best_only=True, \n",
    "                                         mode='min')\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, \n",
    "                                          factor=0.2,\n",
    "                                          patience=2, \n",
    "                                          min_lr=0.00001, \n",
    "                                          epsilon=1e-4, \n",
    "                                          mode='min')\n",
    "            \n",
    "\n",
    "            callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n",
    "            \n",
    "            #model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\n",
    "            model = proposedmodel(sequence_model, \n",
    "                                  sequence_hidden_unit, \n",
    "                                  embed_name, \n",
    "                                  ner_representation_limit,\n",
    "                                  filter_number)\n",
    "            \n",
    "            model.fit([x_train_lstm, x_train_ner], \n",
    "                      y_train[each_problem], \n",
    "                      epochs=num_epoch, \n",
    "                      verbose=0, \n",
    "                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), \n",
    "                      callbacks=callbacks, \n",
    "                      batch_size=batch_size)\n",
    "            \n",
    "            \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            print_scores_cnn(predictions, \n",
    "                             probs, \n",
    "                             y_test[each_problem], \n",
    "                             embed_name, \n",
    "                             each_problem, \n",
    "                             iteration, \n",
    "                             sequence_hidden_unit)\n",
    "            \n",
    "            model.load_weights(best_model_name)\n",
    "                      \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            save_scores_cnn(predictions, \n",
    "                            probs, \n",
    "                            y_test[each_problem], \n",
    "                            embed_name, \n",
    "                            each_problem, \n",
    "                            iteration,\n",
    "                            sequence_hidden_unit, \n",
    "                            sequence_model, \n",
    "                            type_of_ner)\n",
    "            del model\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLH_project_py38] *",
   "language": "python",
   "name": "conda-env-DLH_project_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
