{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes made by kts4 / noami2\n",
    "- `glove` was not available and so needed to install `mittens` instead\n",
    "  - `from mittens import GloVe as glove`\n",
    "- We do not have the `FastText` model and so all references to it and combined model have to be commented out\n",
    "- The calls to access the `Word2vec` and `FastText` models needed to be updated\n",
    "  - `w2vec[...]` changed to `w2vec.wv[...]` and `fasttext[...]` changed to `fasttext.wv[...]`\n",
    "- In Python 3 you can no longer convert from `map` to `np.array`\n",
    "  - So, `t = np.asarray(map(mean, zip(*avg)))` changed to `t = np.asarray(list(map(mean, zip(*avg))))`\n",
    "- Accessing `FastText` elements raised an `IndexError`\n",
    "  - These calls are now wraped in a `try` block\n",
    "- `new_word2vec_dict` was never initialised\n",
    "  - Should be a copy of `new_word2vec` with commonalities between it and `FastText` model removed\n",
    "  - Need to ensure the `diff` set is actually useful\n",
    "- Changes to newlines / spacings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "from tqdm import tqdm\n",
    "# import glove\n",
    "# from glove import Corpus\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify what to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_word2vec = True\n",
    "run_fastText = True\n",
    "run_combined = True\n",
    "run_blueBERT = False\n",
    "run_clinicalBERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "\n",
    "if run_word2vec: w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "if run_fastText: fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if run_blueBERT:\n",
    "    bluebert_tokenizer = AutoTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "    bluebert_model = AutoModel.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "\n",
    "if run_clinicalBERT:\n",
    "    clinicalbert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    clinicalbert_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_word2vec:\n",
    "    print(\"w2vec starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_word2vec = {}\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(w2vec.wv[i[0]])\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            continue\n",
    "                        avg = np.asarray(avg)\n",
    "                        t = np.asarray(list(map(mean, zip(*avg))))\n",
    "                        patient_temp.append(t)\n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_word2vec[k] = patient_temp\n",
    "\n",
    "    print(\"w2vec finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_fastText:\n",
    "    print(\"fasttext starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_fasttextvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(fasttext.wv[i[0]])\n",
    "                except:\n",
    "                    pass\n",
    "            if len(patient_temp) == 0: continue\n",
    "            new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    print(\"fasttext finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Combined - Word2Vec & FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_combined:\n",
    "    print(\"combined starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_concatvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "            patient_temp = []\n",
    "        #     if k != 6: continue\n",
    "            for i in v:\n",
    "                w2vec_temp = []\n",
    "                try:\n",
    "                    w2vec_temp = w2vec.wv[i[0]]\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            w2vec_temp = [0] * 100\n",
    "                        else:\n",
    "                            avg = np.asarray(avg)\n",
    "                            w2vec_temp = np.asarray(list(map(mean, zip(*avg))))\n",
    "                    else:\n",
    "                        w2vec_temp = [0] * 100\n",
    "\n",
    "                try:\n",
    "                    fasttemp = fasttext.wv[i[0]]\n",
    "                    appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "                except:\n",
    "                    appended = np.append(w2vec_temp, 0)\n",
    "\n",
    "                # appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "                patient_temp.append(appended)\n",
    "            if len(patient_temp) == 0: continue\n",
    "            new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(\"combined finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BlueBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_blueBERT:\n",
    "    print(\"BlueBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_bluebert = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "            patient_temp = []\n",
    "            for i in v:\n",
    "                # try:\n",
    "                input_ids = bluebert_tokenizer.encode(i[0], add_special_tokens=True)\n",
    "                embeddings = bluebert_model(torch.tensor([input_ids]))[0][0][1:-1]\n",
    "                patient_temp.append(embeddings)\n",
    "                # except:\n",
    "                    # pass\n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_bluebert[k] = patient_temp\n",
    "\n",
    "    print(\"BlueBERT finished\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_clinicalBERT:\n",
    "    print(\"ClinicalBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_clinicalbert = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "            patient_temp = []\n",
    "            for i in v:\n",
    "                # try:\n",
    "                input_ids = clinicalbert_tokenizer.encode(i[0], add_special_tokens=True)\n",
    "                embeddings = clinicalbert_model(torch.tensor([input_ids]))[0][0][1:-1]\n",
    "                patient_temp.append(embeddings)\n",
    "                # except:\n",
    "                    # pass\n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_clinicalbert[k] = patient_temp\n",
    "\n",
    "    print(\"ClinicalBERT finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec), len(new_bluebert), len(new_clinicalbert))\n",
    "if run_word2vec:     pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n",
    "if run_fastText:     pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n",
    "if run_combined:     pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")\n",
    "if run_blueBERT:     pd.to_pickle(new_bluebert, \"data/\"+names+\"_bluebert_dict.pkl\")\n",
    "if run_clinicalBERT: pd.to_pickle(new_clinicalbert, \"data/\"+names+\"_clinicalbert_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_word2vec and run_fastText and run_combined:\n",
    "\n",
    "    new_fasttextvec_keys = set(new_fasttextvec.keys())\n",
    "    new_word2vec_keys    = set(new_word2vec.keys())\n",
    "    new_concatvec_keys   = set(new_concatvec.keys())\n",
    "    intersection_keys    = new_fasttextvec_keys.intersection(new_word2vec_keys).intersection(new_concatvec_keys)\n",
    "\n",
    "    print(\"Lengths before: {}, {}, {}\".format(len(new_word2vec), len(new_fasttextvec), len(new_concatvec)))\n",
    "\n",
    "    for i in new_fasttextvec_keys - intersection_keys:\n",
    "        del new_fasttextvec[i]\n",
    "    for i in new_word2vec_keys - intersection_keys:\n",
    "        del new_word2vec[i]\n",
    "    for i in new_concatvec_keys - intersection_keys:\n",
    "        del new_concatvec[i]\n",
    "\n",
    "    print(\"Lengths after:  {}, {}, {}\".format(len(new_word2vec), len(new_fasttextvec), len(new_concatvec)))\n",
    "\n",
    "    pd.to_pickle(new_word2vec, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n",
    "    pd.to_pickle(new_fasttextvec, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n",
    "    pd.to_pickle(new_concatvec, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")\n",
    "    print(\"done with writing the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:DLH_project_py38] *",
   "language": "python",
   "name": "conda-env-DLH_project_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
