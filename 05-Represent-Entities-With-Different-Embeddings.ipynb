{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes made by kts4 / noami2\n",
    "- `glove` was not available and so needed to install `mittens` instead\n",
    "  - `from mittens import GloVe as glove`\n",
    "- We do not have the `FastText` model and so all references to it and combined model have to be commented out\n",
    "- The calls to access the `Word2vec` and `FastText` models needed to be updated\n",
    "  - `w2vec[...]` changed to `w2vec.wv[...]` and `fasttext[...]` changed to `fasttext.wv[...]`\n",
    "- In Python 3 you can no longer convert from `map` to `np.array`\n",
    "  - So, `t = np.asarray(map(mean, zip(*avg)))` changed to `t = np.asarray(list(map(mean, zip(*avg))))`\n",
    "- Accessing `FastText` elements raised an `IndexError`\n",
    "  - These calls are now wraped in a `try` block\n",
    "- `new_word2vec_dict` was never initialised\n",
    "  - Should be a copy of `new_word2vec` with commonalities between it and `FastText` model removed\n",
    "  - Need to ensure the `diff` set is actually useful\n",
    "- Changes to newlines / spacings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify what to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_word2vec = True\n",
    "run_fastText = True\n",
    "run_combined = True\n",
    "run_blueBERT = False\n",
    "run_clinicalBERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "\n",
    "if run_word2vec: w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "if run_fastText: fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_word2vec:\n",
    "    print(\"w2vec starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_word2vec = {}\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(w2vec.wv[i[0]])\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            continue\n",
    "                        avg = np.asarray(avg)\n",
    "                        t = np.asarray(list(map(mean, zip(*avg))))\n",
    "                        patient_temp.append(t)\n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_word2vec[k] = patient_temp\n",
    "\n",
    "    print(\"w2vec finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_fastText:\n",
    "    print(\"fasttext starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_fasttextvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "\n",
    "            patient_temp = []\n",
    "\n",
    "            for i in v:\n",
    "                try:\n",
    "                    patient_temp.append(fasttext.wv[i[0]])\n",
    "                except:\n",
    "                    pass\n",
    "            if len(patient_temp) == 0: continue\n",
    "            new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    print(\"fasttext finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Combined - Word2Vec & FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_combined:\n",
    "    print(\"combined starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_concatvec = {}\n",
    "\n",
    "        for k,v in tqdm(data.items()):\n",
    "            \n",
    "            patient_temp = []\n",
    "        #     if k != 6: continue\n",
    "            for i in v:\n",
    "                w2vec_temp = []\n",
    "                try:\n",
    "                    w2vec_temp = w2vec.wv[i[0]]\n",
    "                except:\n",
    "                    avg = []\n",
    "                    num = 0\n",
    "                    temp = []\n",
    "\n",
    "                    if len(i[0].split(\" \")) > 1:\n",
    "                        for each_word in i[0].split(\" \"):\n",
    "                            try:\n",
    "                                temp = w2vec.wv[each_word]\n",
    "                                avg.append(temp)\n",
    "                                num += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                        if num == 0: \n",
    "                            w2vec_temp = [0] * 100\n",
    "                        elif num ==1:\n",
    "                            w2vec_temp = temp\n",
    "                        else:\n",
    "                            avg = np.array(avg)\n",
    "                            w2vec_temp = avg.mean(axis=0)\n",
    "                    else:\n",
    "                        w2vec_temp = [0] * 100\n",
    "\n",
    "                try:\n",
    "                    fasttemp = fasttext.wv[i[0]]\n",
    "                except:\n",
    "                    fasttemp = [0] * 100\n",
    "                    \n",
    "                appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "                patient_temp.append(appended)\n",
    "                \n",
    "            if len(patient_temp) == 0: \n",
    "                continue\n",
    "            new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(\"combined finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BlueBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_blueBERT:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "    model = AutoModel.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")\n",
    "\n",
    "    batch_size = 128\n",
    "    patient_embeddings = []\n",
    "    \n",
    "    print(\"BlueBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_bluebert = {}\n",
    "    \n",
    "        for k, v in tqdm(data.items()):\n",
    "            # Batch input texts\n",
    "            input_texts = [i[0] for i in v]\n",
    "            encoded_inputs = tokenizer.batch_encode_plus(input_texts,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         return_tensors='pt',\n",
    "                                                         padding=True,\n",
    "                                                         truncation=True)\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_inputs).last_hidden_state\n",
    "            \n",
    "            # Extract [CLS] embeddings for each input text\n",
    "            embeddings = output[:, 0, :].tolist()\n",
    "            \n",
    "            if len(embeddings) == 0: \n",
    "                continue\n",
    "            new_bluebert[k] = embeddings\n",
    "        \n",
    "    # Convert values to NumPy arrays and store back in dictionary\n",
    "    for key in new_bluebert:\n",
    "        new_bluebert[key] = np.array(new_bluebert[key])\n",
    "    \n",
    "    print(\"BlueBERT finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_clinicalBERT:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "    model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "    batch_size = 128\n",
    "    patient_embeddings = []\n",
    "\n",
    "    print(\"ClinicalBERT starting..\")\n",
    "    for data, names in zip(data_types, data_names):\n",
    "   \n",
    "        new_clinicalbert = {}\n",
    "    \n",
    "        for k, v in tqdm(data.items()):\n",
    "            # Batch input texts\n",
    "            input_texts = [i[0] for i in v]\n",
    "            encoded_inputs = tokenizer.batch_encode_plus(input_texts,\n",
    "                                                         add_special_tokens=True,\n",
    "                                                         return_tensors='pt',\n",
    "                                                         padding=True,\n",
    "                                                         truncation=True)\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded_inputs).last_hidden_state\n",
    "\n",
    "            \n",
    "            # Extract [CLS] embeddings for each input text\n",
    "            embeddings = output[:, 0, :].tolist()\n",
    "            \n",
    "            if len(embeddings) == 0: \n",
    "                continue\n",
    "            new_clinicalbert[k] = embeddings\n",
    "        \n",
    "    # Convert values to NumPy arrays and store back in dictionary\n",
    "    for key in new_clinicalbert:\n",
    "        new_clinicalbert[key] = np.array(new_clinicalbert[key])\n",
    "    \n",
    "    print(\"ClinicalBERT finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_word2vec:     pd.to_pickle(new_word2vec,     \"data/new_ner_word2vec_dict.pkl\")\n",
    "if run_fastText:     pd.to_pickle(new_fasttextvec,  \"data/new_ner_fasttext_dict.pkl\")\n",
    "if run_combined:     pd.to_pickle(new_concatvec,    \"data/new_ner_combined_dict.pkl\")\n",
    "if run_blueBERT:     pd.to_pickle(new_bluebert,     \"data/new_ner_bluebert_dict.pkl\")\n",
    "if run_clinicalBERT: pd.to_pickle(new_clinicalbert, \"data/new_ner_clinicalbert_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word2vec     = pd.read_pickle(\"data/new_ner_word2vec_dict.pkl\")\n",
    "print(\"new_word2vec loaded\")\n",
    "new_fasttextvec  = pd.read_pickle(\"data/new_ner_fasttext_dict.pkl\")\n",
    "print(\"new_fasttextvec loaded\")\n",
    "new_concatvec    = pd.read_pickle(\"data/new_ner_combined_dict.pkl\")\n",
    "print(\"new_concatvec loaded\")\n",
    "# new_bluebertvec  = pd.read_pickle(\"data/new_ner_bluebert_dict.pkl\")\n",
    "# print(\"new_bluebert loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_word2vec and run_fastText and run_combined:\n",
    "\n",
    "    new_fasttextvec_keys  = set(new_fasttextvec.keys())\n",
    "    new_word2vec_keys     = set(new_word2vec.keys())\n",
    "    new_concatvec_keys    = set(new_concatvec.keys())\n",
    "    intersection_keys     = new_fasttextvec_keys.intersection(new_word2vec_keys).intersection(new_concatvec_keys)\n",
    "\n",
    "    print(\"Lengths before: {}, {}, {}\".format(len(new_word2vec), \n",
    "                                                  len(new_fasttextvec), \n",
    "                                                  len(new_concatvec)))\n",
    "\n",
    "    for i in new_fasttextvec_keys - intersection_keys:\n",
    "        del new_fasttextvec[i]\n",
    "    for i in new_word2vec_keys - intersection_keys:\n",
    "        del new_word2vec[i]\n",
    "    for i in new_concatvec_keys - intersection_keys:\n",
    "        del new_concatvec[i]\n",
    "\n",
    "    print(\"Lengths after:  {}, {}, {}\".format(len(new_word2vec), \n",
    "                                              len(new_fasttextvec), \n",
    "                                              len(new_concatvec)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(new_word2vec,    \"data/new_ner_word2vec_limited_dict.pkl\")\n",
    "print(\"new_word2vec saved\")\n",
    "pd.to_pickle(new_fasttextvec, \"data/new_ner_fasttext_limited_dict.pkl\")\n",
    "print(\"new_fasttextvec saved\")\n",
    "pd.to_pickle(new_concatvec,   \"data/new_ner_combined_limited_dict.pkl\")\n",
    "print(\"new_concatvec saved\")\n",
    "\n",
    "print(\"done with writing the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:DLH_project_py38] *",
   "language": "python",
   "name": "conda-env-DLH_project_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
