{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f45fe",
   "metadata": {},
   "source": [
    "## Results from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92753672",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU = {\n",
    "    'mort_hosp':{'auc': 0.8504, 'auprc': 0.5215, 'F1': 0.4229},\n",
    "    'mort_icu': {'auc': 0.8632, 'auprc': 0.4651, 'F1': 0.3630},\n",
    "    'los_3':    {'auc': 0.6740, 'auprc': 0.6017, 'F1': 0.5336},\n",
    "    'los_7':    {'auc': 0.7054, 'auprc': 0.1625, 'F1': 0.0233},\n",
    "}\n",
    "\n",
    "# Averaged Multimodal\n",
    "\n",
    "word2vec_avg = {\n",
    "    'mort_hosp':{'auc': 0.8642, 'auprc': 0.5422, 'F1': 0.4542},\n",
    "    'mort_icu': {'auc': 0.8717, 'auprc': 0.4847, 'F1': 0.4230},\n",
    "    'los_3':    {'auc': 0.6863, 'auprc': 0.6181, 'F1': 0.5419},\n",
    "    'los_7':    {'auc': 0.7159, 'auprc': 0.1791, 'F1': 0.0135},\n",
    "}\n",
    "\n",
    "fastText_avg = {\n",
    "    'mort_hosp':{'auc': 0.8609, 'auprc': 0.5447, 'F1': 0.4550},\n",
    "    'mort_icu': {'auc': 0.8714, 'auprc': 0.4836, 'F1': 0.4291},\n",
    "    'los_3':    {'auc': 0.6855, 'auprc': 0.6159, 'F1': 0.5446},\n",
    "    'los_7':    {'auc': 0.7131, 'auprc': 0.1757, 'F1': 0.0102},\n",
    "}\n",
    "\n",
    "concat_avg = {\n",
    "    'mort_hosp':{'auc': 0.8598, 'auprc': 0.5419, 'F1': 0.4566},\n",
    "    'mort_icu': {'auc': 0.8690, 'auprc': 0.4828, 'F1': 0.4076},\n",
    "    'los_3':    {'auc': 0.6861, 'auprc': 0.6169, 'F1': 0.5470},\n",
    "    'los_7':    {'auc': 0.7159, 'auprc': 0.1767, 'F1': 0.0137},\n",
    "}\n",
    "\n",
    "\n",
    "# Proposed Model\n",
    "\n",
    "word2vec_proposed = {\n",
    "    'mort_hosp':{'auc': 0.8755, 'auprc': 0.5587, 'F1': 0.4723},\n",
    "    'mort_icu': {'auc': 0.8835, 'auprc': 0.4923, 'F1': 0.4302},\n",
    "    'los_3':    {'auc': 0.6954, 'auprc': 0.6268, 'F1': 0.5504},\n",
    "    'los_7':    {'auc': 0.7255, 'auprc': 0.1878, 'F1': 0.0158},\n",
    "}\n",
    "\n",
    "fastText_proposed = {\n",
    "    'mort_hosp':{'auc': 0.8715, 'auprc': 0.5568, 'F1': 0.4687},\n",
    "    'mort_icu': {'auc': 0.8785, 'auprc': 0.4878, 'F1': 0.4309},\n",
    "    'los_3':    {'auc': 0.6961, 'auprc': 0.6255, 'F1': 0.5587},\n",
    "    'los_7':    {'auc': 0.7181, 'auprc': 0.1801, 'F1': 0.0108},\n",
    "}\n",
    "\n",
    "concat_proposed = {\n",
    "    'mort_hosp':{'auc': 0.8698, 'auprc': 0.5535, 'F1': 0.4638},\n",
    "    'mort_icu': {'auc': 0.8766, 'auprc': 0.4874, 'F1': 0.4224},\n",
    "    'los_3':    {'auc': 0.6993, 'auprc': 0.6277, 'F1': 0.5582},\n",
    "    'los_7':    {'auc': 0.7192, 'auprc': 0.1825, 'F1': 0.0138},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bceace",
   "metadata": {},
   "source": [
    "Place them into pandas df to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2246d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_results_07\n",
    "paper_results_07 = pd.DataFrame.from_dict(GRU, orient='index')\n",
    "paper_results_07 = paper_results_07.reset_index()\n",
    "paper_results_07.rename(columns={'index':'task'}, inplace=True)\n",
    "paper_results_07['layer'] = 'GRU'\n",
    "\n",
    "# paper_results_08\n",
    "word2vec_avg_pd = pd.DataFrame.from_dict(word2vec_avg, orient='index')\n",
    "word2vec_avg_pd = word2vec_avg_pd.reset_index()\n",
    "word2vec_avg_pd.rename(columns={'index':'task'}, inplace=True)\n",
    "word2vec_avg_pd['embedding'] = 'word2vec'\n",
    "\n",
    "fastText_avg_pd = pd.DataFrame.from_dict(fastText_avg, orient='index')\n",
    "fastText_avg_pd = fastText_avg_pd.reset_index()\n",
    "fastText_avg_pd.rename(columns={'index':'task'}, inplace=True)\n",
    "fastText_avg_pd['embedding'] = 'fasttext'\n",
    "\n",
    "concat_avg = pd.DataFrame.from_dict(word2vec_avg, orient='index')\n",
    "concat_avg = concat_avg.reset_index()\n",
    "concat_avg.rename(columns={'index':'task'}, inplace=True)\n",
    "concat_avg['embedding'] = 'concat'\n",
    "\n",
    "paper_results_08 = pd.concat([word2vec_avg_pd, fastText_avg_pd, concat_avg])\n",
    "\n",
    "# paper_results_09\n",
    "word2vec_proposed_pd = pd.DataFrame.from_dict(word2vec_proposed, orient='index')\n",
    "word2vec_proposed_pd = word2vec_proposed_pd.reset_index()\n",
    "word2vec_proposed_pd.rename(columns={'index':'task'}, inplace=True)\n",
    "word2vec_proposed_pd['embedding'] = 'word2vec'\n",
    "\n",
    "fastText_proposed_pd = pd.DataFrame.from_dict(fastText_proposed, orient='index')\n",
    "fastText_proposed_pd = fastText_proposed_pd.reset_index()\n",
    "fastText_proposed_pd.rename(columns={'index':'task'}, inplace=True)\n",
    "fastText_proposed_pd['embedding'] = 'fasttext'\n",
    "\n",
    "concat_proposed_pd = pd.DataFrame.from_dict(concat_proposed, orient='index')\n",
    "concat_proposed_pd = concat_proposed_pd.reset_index()\n",
    "concat_proposed_pd.rename(columns={'index':'task'}, inplace=True)\n",
    "concat_proposed_pd['embedding'] = 'concat'\n",
    "\n",
    "paper_results_09 = pd.concat([word2vec_avg_pd, fastText_avg_pd, concat_avg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df582d",
   "metadata": {},
   "source": [
    "## Functions to produce tabular outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add n blank rows\n",
    "def add_blank_rows_df(df, after_each_n):\n",
    "    new_index = pd.RangeIndex(len(df) + np.ceil(len(df)/after_each_n))\n",
    "    new_df = pd.DataFrame('', index=new_index, columns=df.columns)\n",
    "    \n",
    "    ids = []\n",
    "    jump = 0\n",
    "    for i in range(0, len(df)):\n",
    "        ids.append(i + np.floor(i/after_each_n))\n",
    "        \n",
    "    new_df.loc[ids] = df.values\n",
    "    return new_df\n",
    "\n",
    "def get_results(result_path: str,\n",
    "                paper_results,\n",
    "                model_category: str,\n",
    "                sort_by:str,\n",
    "                group_by: list,\n",
    "                exp_naming: list,\n",
    "                var_type: str,\n",
    "                show_rankings: bool = True):\n",
    "    \n",
    "    result_dists = []\n",
    "    exps = os.listdir(result_path)\n",
    "    \n",
    "    # Pulls the results from the `.p` files \n",
    "    for exp in exps:\n",
    "        if '.p' not in exp:\n",
    "            continue\n",
    "        result_dict = pd.read_pickle(os.path.join(result_path, exp))\n",
    "        result_dict[exp_naming[0]], result_dict[exp_naming[1]], result_dict[exp_naming[2]], result_dict[exp_naming[3]] = exp.split(\"-\")[:4]\n",
    "        result_dict[\"model_category\"] = model_category\n",
    "        result_dists.append(result_dict)\n",
    "        \n",
    "    result = pd.DataFrame(result_dists)\n",
    "    result.sort_values(by=['task', sort_by], inplace=True, ascending=False)\n",
    "\n",
    "    # Generate the summary results table\n",
    "    summary_results, df_all = get_summary_results(result, \n",
    "                                          model_category,\n",
    "                                          sort_by,\n",
    "                                          group_by,\n",
    "                                          exp_naming,\n",
    "                                          var_type,\n",
    "                                          show_rankings)\n",
    "    \n",
    "    \n",
    "    results_difference = get_results_difference(result,\n",
    "                                                paper_results,\n",
    "                                                model_category,\n",
    "                                                sort_by,\n",
    "                                                group_by,\n",
    "                                                exp_naming,\n",
    "                                                var_type)\n",
    "    \n",
    "    return result, summary_results, results_difference, df_all\n",
    "\n",
    "def get_summary_results(result, \n",
    "                        model_category: str,\n",
    "                        sort_by:str,\n",
    "                        group_by: list,\n",
    "                        exp_naming: list,\n",
    "                        var_type: str,\n",
    "                        show_rankings: bool = True):\n",
    "    \n",
    "    df = result.groupby(by=group_by).agg({\"auc\":['mean','std'], \n",
    "                                          \"auprc\":['mean','std'],\n",
    "                                          \"F1\":['mean','std'], \n",
    "                                          'acc':['mean','std']})\n",
    "    df.columns = df.columns.map('_'.join).str.strip('_')\n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"model_category\"] = model_category\n",
    "    \n",
    "    # Get values in terms of %\n",
    "    for col in ['auc_mean', 'auc_std', 'auprc_mean', 'auprc_std',\n",
    "       'F1_mean', 'F1_std', 'acc_mean', 'acc_std']:\n",
    "       df[col] = df[col] * 100\n",
    "    \n",
    "    if show_rankings:\n",
    "        for col in ['auc_mean', 'auprc_mean', 'F1_mean']:\n",
    "            df[f'{col}_ranking'] = df.groupby('task')[col].rank(method='dense', ascending=False).astype(int).apply(lambda x: f'({x})')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Rename the tasks\n",
    "    task_mapping = {\n",
    "        \"los_3\": \"LOS > 3 Days\",\n",
    "        \"los_7\": \"LOS > 7 Days\",\n",
    "        \"mort_hosp\": \"In-Hospital Mortality\",\n",
    "        \"mort_icu\": \"In-ICU Mortality\"\n",
    "    }\n",
    "    for k, v in task_mapping.items():\n",
    "        df.loc[df[\"task\"] == k, \"task\"] = v\n",
    "        \n",
    "    df_all = df.copy()\n",
    "    \n",
    "    # Format the numeric results\n",
    "    for metric in [\"auc\", \"auprc\", \"F1\", \"acc\"]:\n",
    "        df[metric] = df[[f\"{metric}_mean\", f\"{metric}_std\"]].apply(lambda x: \"{:.2f} +/- {:.3f}\".format(x[0], x[1]), axis=1)\n",
    "    \n",
    "    # Order the results in the manner desired\n",
    "    if show_rankings:\n",
    "        df = df[[\"task\", \"model_category\",var_type, \"auc\", \"auc_mean_ranking\", \"auprc\", \"auprc_mean_ranking\", \"F1\", \"F1_mean_ranking\"]]\n",
    "        df.columns = [\"task\", \"model_category\",var_type, \"auc\", \"\", \"auprc\", \"\", \"F1\", \"\"]\n",
    "    else:\n",
    "        df = df[[\"task\", \"model_category\",var_type, \"auc\", \"auprc\", \"F1\"]]\n",
    "        \n",
    "\n",
    "    df.sort_values(by=['task', sort_by], inplace=True, ascending=[True, True])\n",
    "\n",
    "    return df, df_all\n",
    "\n",
    "\n",
    "def get_results_difference(result, \n",
    "                           paper_results,\n",
    "                           model_category: str,\n",
    "                           sort_by:str,\n",
    "                           group_by: list,\n",
    "                           exp_naming: list,\n",
    "                           var_type: str):\n",
    "    \n",
    "    results_difference = result.groupby(by=group_by).agg({\"auc\":['mean'], \"auprc\":['mean'], \"F1\":['mean']})\n",
    "    results_difference = results_difference.rename(columns={'mean':''})\n",
    "    results_difference.columns = results_difference.columns.map('_'.join).str.strip('_')\n",
    "    results_difference.reset_index(inplace=True)\n",
    "    results_difference[\"model_category\"] = model_category\n",
    "    \n",
    "    # Take the difference between the reported results in the paper and what we have recorded [auc, auprc, F1]\n",
    "    results_difference = results_difference.assign(auc = results_difference['auc'] - results_difference.merge(paper_results, \n",
    "                                                                                         on=['task',var_type], \n",
    "                                                                                         suffixes=('','_y'), \n",
    "                                                                                         how='left')['auc_y'])  \n",
    "    results_difference = results_difference.assign(auprc = results_difference['auprc'] - results_difference.merge(paper_results, \n",
    "                                                                                         on=['task',var_type], \n",
    "                                                                                         suffixes=('','_y'), \n",
    "                                                                                         how='left')['auprc_y']) \n",
    "    results_difference = results_difference.assign(F1 = results_difference['F1'] - results_difference.merge(paper_results, \n",
    "                                                                                         on=['task',var_type], \n",
    "                                                                                         suffixes=('','_y'), \n",
    "                                                                                         how='left')['F1_y']) \n",
    "    # If no data from paper, then set to '-'\n",
    "    results_difference.loc[results_difference.merge(paper_results, \n",
    "                                                     on=['task',var_type], \n",
    "                                                     suffixes=('','_y'), \n",
    "                                                     how='left')['auc_y'].isnull(), 'auc'] = '-'\n",
    "    results_difference.loc[results_difference.merge(paper_results, \n",
    "                                                     on=['task',var_type], \n",
    "                                                     suffixes=('','_y'), \n",
    "                                                     how='left')['auprc_y'].isnull(), 'auprc'] = '-'\n",
    "    results_difference.loc[results_difference.merge(paper_results, \n",
    "                                                     on=['task',var_type], \n",
    "                                                     suffixes=('','_y'), \n",
    "                                                     how='left')['F1_y'].isnull(), 'F1'] = '-'\n",
    "\n",
    "\n",
    "    # Get values in terms of %\n",
    "    for col in ['auc', 'auprc', 'F1']:\n",
    "       results_difference[col] = results_difference[col] * 100\n",
    "    \n",
    "    # Rename the tasks\n",
    "    task_mapping = {\n",
    "        \"los_3\": \"LOS > 3 Days\",\n",
    "        \"los_7\": \"LOS > 7 Days\",\n",
    "        \"mort_hosp\": \"In-Hospital Mortality\",\n",
    "        \"mort_icu\": \"In-ICU Mortality\"\n",
    "    }\n",
    "    for k, v in task_mapping.items():\n",
    "        results_difference.loc[results_difference[\"task\"] == k, \"task\"] = v\n",
    "    \n",
    "    # Format the numeric results\n",
    "    for metric in [\"auc\", \"auprc\", \"F1\"]:\n",
    "        results_difference[metric] = results_difference[[metric]].apply(lambda x: \"{:.2f}\".format(x[0]) if not isinstance(x[0], str) else '-', axis=1)\n",
    "    \n",
    "    # Order the results in the manner desired\n",
    "    results_difference = results_difference[[\"task\", \"model_category\",var_type, \"auc\", \"auprc\", \"F1\"]]\n",
    "    \n",
    "    results_difference.sort_values(by=['task', sort_by], inplace=True, ascending=[True, True])\n",
    "    \n",
    "    return results_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409e384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09605c97",
   "metadata": {},
   "source": [
    "# Only time-series data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733df62c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_07, summary_results_07, results_difference_07, _ = get_results(\n",
    "    result_path=\"results/07-baseline/GRU/hidden_units_256\",\n",
    "    paper_results=paper_results_07,\n",
    "    model_category=\"GRU\",\n",
    "    sort_by=\"auc\",\n",
    "    group_by=[\"task\", \"layer\"],\n",
    "    exp_naming=[\"hidden_units\",\"layer\",\"task\",\"iteration\"],\n",
    "    var_type=\"layer\",\n",
    "    show_rankings=False)\n",
    "\n",
    "summary_results_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_difference_07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb315129",
   "metadata": {},
   "source": [
    "# Baseline time-series and NLP data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e799485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_08, summary_results_08, results_difference_08, _ = get_results(\n",
    "    result_path=\"results/08-multimodal\",\n",
    "    paper_results=paper_results_08,\n",
    "    model_category=\"Average Multimodal\",\n",
    "    sort_by=\"embedding\",\n",
    "    group_by=[\"task\", \"embedding\"],\n",
    "    exp_naming=[\"layer\",\"hidden_units\",\"embedding\",\"task\"],\n",
    "    var_type=\"embedding\")\n",
    "\n",
    "summary_results_08_print = add_blank_rows_df(summary_results_08, \n",
    "                                             summary_results_08['task'].value_counts()['In-Hospital Mortality'])\n",
    "summary_results_08_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_difference_08_print = add_blank_rows_df(results_difference_08, \n",
    "                                                results_difference_08['task'].value_counts()['In-Hospital Mortality'])\n",
    "\n",
    "results_difference_08_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801273d4",
   "metadata": {},
   "source": [
    "# Proposed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_09, summary_results_09, results_difference_09, df_all = get_results(\n",
    "    result_path=\"results/09-cnn\",\n",
    "    paper_results=paper_results_09,\n",
    "    model_category=\"Proposed Model\",\n",
    "    sort_by=\"embedding\",\n",
    "    group_by=[\"task\", \"embedding\"],\n",
    "    exp_naming=[\"sequence_name\",\"hidden_unit_size\",\"embedding\",\"task\"],\n",
    "    var_type=\"embedding\")\n",
    "\n",
    "summary_results_09_print = add_blank_rows_df(summary_results_09, \n",
    "                                             summary_results_09['task'].value_counts()['In-Hospital Mortality'])\n",
    "summary_results_09_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_difference_09_print = add_blank_rows_df(results_difference_09, \n",
    "                                                results_difference_09['task'].value_counts()['In-Hospital Mortality'])\n",
    "\n",
    "results_difference_09_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf726f1f",
   "metadata": {},
   "source": [
    "# Functions to make tables look like those from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_3(df1, df2, \n",
    "            orig_paper_results:bool):\n",
    "    \n",
    "    \n",
    "    # Make sure the columns match between the two df and concat\n",
    "    cols = [\"task\", \"model_category\",'embedding', \"auc\", \"auprc\", \"F1\"]\n",
    "    df1 = df1[df1.columns.intersection(cols)]\n",
    "    df2 = df2[df2.columns.intersection(cols)]\n",
    "    df = pd.concat([df1, df2])\n",
    "    \n",
    "    # Only keep the embeddings from the paper\n",
    "    if orig_paper_results:\n",
    "        rows = [np.nan, 'word2vec', 'fasttext', 'concat']\n",
    "        df = df[df['embedding'].isin(rows)]\n",
    "    \n",
    "    # Set the GRU embedding to \"\"\n",
    "    df['embedding'].fillna(\"-\", inplace=True)\n",
    "    \n",
    "    # Add the rankings\n",
    "    for col in ['auc', 'auprc', 'F1']:\n",
    "        df[f'{col}_ranking'] = df.groupby('task')[col].rank(method='dense', ascending=False).astype(int).apply(lambda x: f'({x})')\n",
    "    \n",
    "    # Select the column order and then remove the ranking headings\n",
    "    df = df[[\"task\", \"model_category\",'embedding', \"auc\", \"auc_ranking\", \"auprc\", \"auprc_ranking\", \"F1\", \"F1_ranking\"]]\n",
    "    df.columns = [\"task\", \"model_category\",'embedding', \"auc\", \"\", \"auprc\", \"\", \"F1\", \"\"]\n",
    "    \n",
    "    # Sort the df\n",
    "    df.sort_values(by=['task', 'model_category', 'embedding'], inplace=True, ascending=[True, False, True])\n",
    "    \n",
    "    # Add a space after each group\n",
    "    df = add_blank_rows_df(df, df['task'].value_counts()['In-Hospital Mortality'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeef4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_model(df1, df2,\n",
    "                         orig_paper_results:bool):\n",
    "    tasks = [\"los_3\",\"los_7\",\"mort_hosp\", \"mort_icu\" ]\n",
    "\n",
    "    df = pd.concat([df1, df2]).fillna('-')\n",
    "    \n",
    "    # Only keep the embeddings from the paper\n",
    "    if orig_paper_results:\n",
    "        rows = ['-', 'word2vec', 'fasttext', 'concat']\n",
    "        df = df[df['embedding'].isin(rows)]\n",
    "\n",
    "    df = df.groupby(by=['task', 'embedding']).agg({\"auc\":['mean', 'std'], \n",
    "                                                                         \"auprc\":['mean', 'std'],\n",
    "                                                                         \"F1\":['mean', 'std']})\n",
    "    df.columns = df.columns.map('_'.join).str.strip('_')\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    df_auc = df.loc[df.groupby(\"task\")[\"auc_mean\"].idxmax()]\n",
    "    df_auc = df_auc[[\"task\", \"auc_mean\", 'auc_std']]\n",
    "\n",
    "    df_auprc = df.loc[df.groupby(\"task\")[\"auprc_mean\"].idxmax()]\n",
    "    df_auprc = df_auprc[[\"task\", \"auprc_mean\", 'auprc_std']]\n",
    "\n",
    "    df_F1 = df.loc[df.groupby(\"task\")[\"F1_mean\"].idxmax()]\n",
    "    df_F1 = df_F1[[\"task\", \"F1_mean\", 'F1_std']]\n",
    "\n",
    "    df = df_auc.merge(df_auprc, on='task').merge(df_F1, on='task')\n",
    "\n",
    "\n",
    "    # Get values in terms of %\n",
    "    for col in ['auc_mean', 'auc_std', 'auprc_mean', 'auprc_std', 'F1_mean', 'F1_std']:\n",
    "       df[col] = df[col] * 100\n",
    "\n",
    "    # Rename the tasks\n",
    "    task_mapping = {\n",
    "        \"los_3\": \"LOS > 3 Days\",\n",
    "        \"los_7\": \"LOS > 7 Days\",\n",
    "        \"mort_hosp\": \"In-Hospital Mortality\",\n",
    "        \"mort_icu\": \"In-ICU Mortality\"\n",
    "    }\n",
    "    for k, v in task_mapping.items():\n",
    "        df.loc[df[\"task\"] == k, \"task\"] = v\n",
    "\n",
    "    # Format the numeric results\n",
    "    for metric in [\"auc\", \"auprc\", \"F1\"]:\n",
    "        df[metric] = df[[f\"{metric}_mean\", f\"{metric}_std\"]].apply(lambda x: \"{:.2f} +/- {:.3f}\".format(x[0], x[1]), axis=1)\n",
    "\n",
    "    df[\"model_category\"] = 'Best Baseline'\n",
    "    df[\"embedding\"] = '-'\n",
    "\n",
    "    # Order the results in the manner desired\n",
    "    df = df[[\"task\", \"model_category\", 'embedding', \"auc\", \"auprc\", \"F1\"]]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_4(df1, df2,\n",
    "           orig_paper_results:bool):\n",
    "\n",
    "    cols = [\"task\", \"model_category\",'embedding', \"auc\", \"auprc\", \"F1\"]\n",
    "    df1 = df1[df1.columns.intersection(cols)]\n",
    "    df2 = df2[df2.columns.intersection(cols)]\n",
    "    df = pd.concat([df1, df2])\n",
    "    \n",
    "    # Only keep the embeddings from the paper\n",
    "    if orig_paper_results:\n",
    "        rows = ['-', 'word2vec', 'fasttext', 'concat']\n",
    "        df = df[df['embedding'].isin(rows)]\n",
    "    \n",
    "    for col in ['auc', 'auprc', 'F1']:\n",
    "        df[f'{col}_ranking'] = df.groupby('task')[col].rank(method='dense', ascending=False).astype(int).apply(lambda x: f'({x})')\n",
    "    \n",
    "    df = df[[\"task\", \"model_category\",'embedding', \"auc\", \"auc_ranking\", \"auprc\", \"auprc_ranking\", \"F1\", \"F1_ranking\"]]\n",
    "    df.columns = [\"task\", \"model_category\",'embedding', \"auc\", \"\", \"auprc\", \"\", \"F1\", \"\"]\n",
    "    df.sort_values(by=['task', 'model_category', 'embedding'], inplace=True, ascending=True)\n",
    "    df = add_blank_rows_df(df, df['task'].value_counts()['In-Hospital Mortality'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ec03b",
   "metadata": {},
   "source": [
    "## Table 3 (replicated) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results_paper_1 = table_3(summary_results_07, summary_results_08, True)\n",
    "summary_results_paper_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b0607",
   "metadata": {},
   "source": [
    "## Difference between Table 3 and replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a98628",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_difference_paper_1 = table_3(results_difference_07, results_difference_08, True)\n",
    "results_difference_paper_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce77a9",
   "metadata": {},
   "source": [
    "## Table 4 (replicated) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e3123",
   "metadata": {},
   "source": [
    "First determine the best baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_baseline = determine_best_model(results_07, results_08, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results_paper_2 = table_4(best_baseline, summary_results_09, True)\n",
    "summary_results_paper_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_difference_paper_2 = table_4(results_difference_07, results_difference_08, True)\n",
    "results_difference_paper_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basline Models\")\n",
    "\n",
    "print(\"AUC Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_07['auc'], \n",
    "                                                             errors='coerce')), 2)))\n",
    "print(\"AUPRC Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_07['auprc'], \n",
    "                                                               errors='coerce')), 2)))\n",
    "print(\"F1 Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_07['F1'], \n",
    "                                                            errors='coerce')), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proposed Models\")\n",
    "print(\"AUC Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_08['auc'], \n",
    "                                                             errors='coerce')), 2)))\n",
    "print(\"AUPRC Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_08['auprc'], \n",
    "                                                               errors='coerce')), 2)))\n",
    "print(\"F1 Mean Difference: {}\".format(np.round(np.mean(pd.to_numeric(results_difference_08['F1'], \n",
    "                                                            errors='coerce')), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bcc34",
   "metadata": {},
   "source": [
    "### Latex table output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary_results_paper_1.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae396342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary_results_paper_2.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20244138",
   "metadata": {},
   "source": [
    "## Results with the Ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e383a1b",
   "metadata": {},
   "source": [
    "### Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65007068",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results_paper_1_ablations = table_3(summary_results_07, summary_results_08, False)\n",
    "summary_results_paper_1_ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6b0d8",
   "metadata": {},
   "source": [
    "### Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10193837",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_baseline_ablations = determine_best_model(results_07, results_08, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results_paper_2_ablations = table_4(best_baseline, summary_results_09, False)\n",
    "summary_results_paper_2_ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065c2cd",
   "metadata": {},
   "source": [
    "### Latex table output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary_results_paper_1_ablations.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_results_paper_2_ablations.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae50dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(axs,\n",
    "                 x_vals, \n",
    "                 mean, \n",
    "                 std,\n",
    "                 row,\n",
    "                 col):\n",
    "    \n",
    "    colors = {'word2vec': 'red', \n",
    "          'fasttext': 'red', \n",
    "          'concat': 'red',\n",
    "          'blueBERT': 'blue', \n",
    "          'clinicalBERT': 'blue', \n",
    "          'concat_cnn_L5': 'blue', \n",
    "          'fasttext_cnn_L5': 'blue', \n",
    "          'word2vec_cnn_L5': 'blue'}\n",
    "    \n",
    "    axs[row, col].errorbar(x_vals, \n",
    "                            mean, \n",
    "                            std, \n",
    "                            fmt='o',   \n",
    "                            color='none',\n",
    "                            ecolor=x_vals.map(colors))\n",
    "    axs[row, col].scatter(x_vals, \n",
    "                           mean,\n",
    "                           marker='_', \n",
    "                           color=x_vals.map(colors), \n",
    "                           s=100)\n",
    "    \n",
    "    return axs\n",
    "\n",
    "def update_ticks(ax, row, col, data, group, no_title):\n",
    "    names = ['BlueBERT', 'ClinicalBERT', \n",
    "             'concat', 'concat (a)', \n",
    "             'FastText', 'FastText (a)', \n",
    "             'word2vec', 'word2vec (a)']\n",
    "    \n",
    "\n",
    "    if not no_title:\n",
    "        axs[row, col].set_title('{}'.format(group))\n",
    "\n",
    "    # Remove ytick labels\n",
    "    axs[row, col].tick_params(axis='y', which='both', labelleft=False, labelright=False)\n",
    "\n",
    "    # Only show xticks on the last subplot and rotate them 90 degrees\n",
    "    if row == 2:\n",
    "        axs[row, col].set_xticks(list(range(0,data.shape[0])))\n",
    "        axs[row, col].set_xticklabels(names, rotation=90, fontsize=16) #, ha=\"right\")\n",
    "    else:\n",
    "        axs[row, col].set_xticks([])\n",
    "        \n",
    "    return ax\n",
    "        \n",
    "\n",
    "\n",
    "# Create a plot with mean and standard deviation indicators for each group\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(10, 6))\n",
    "\n",
    "row, col = 0, 0\n",
    "\n",
    "# Group the DataFrame by the 'Group' column\n",
    "groups = df_all.groupby('task')\n",
    "\n",
    "for i, (group, data) in enumerate(groups):\n",
    "        \n",
    "    x_vals = data['embedding']\n",
    "    \n",
    "    ## --------\n",
    "    # AUC    \n",
    "    ## --------\n",
    "    \n",
    "    mean = data['auc_mean'].values\n",
    "    std = data['auc_std'].values\n",
    "    \n",
    "    axs = create_plots(axs, x_vals, mean, std, row, col)\n",
    "    axs = update_ticks(axs, row, col, data, group, no_title=False)\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('AUC', fontsize=16)\n",
    "    \n",
    "    row +=1\n",
    "    \n",
    "    ## --------\n",
    "    # AUPRC    \n",
    "    ## --------\n",
    "    \n",
    "    mean = data['auprc_mean'].values\n",
    "    std = data['auprc_std'].values\n",
    "\n",
    "    axs = create_plots(axs, x_vals, mean, std, row, col)\n",
    "    axs = update_ticks(axs, row, col, data, group, no_title=True)\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('AUPRC', fontsize=16)\n",
    "    \n",
    "    row +=1\n",
    "    \n",
    "    ## --------\n",
    "    # F1    \n",
    "    ## --------\n",
    "    \n",
    "    mean = data['F1_mean'].values\n",
    "    std = data['F1_std'].values\n",
    "\n",
    "    axs = create_plots(axs, x_vals, mean, std, row, col)\n",
    "    axs = update_ticks(axs, row, col, data, group, no_title=True)\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('F1', fontsize=16)\n",
    "    \n",
    "    row +=1\n",
    "    \n",
    "    # Ensure we move on to the right next plot\n",
    "    if row > 2:\n",
    "        row = 0\n",
    "        col +=1\n",
    "        \n",
    "    \n",
    "    if col > 3:\n",
    "        col = 0\n",
    "\n",
    "plt.savefig('images/ablation.png', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff3cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:DLH_project_py38] *",
   "language": "python",
   "name": "conda-env-DLH_project_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
