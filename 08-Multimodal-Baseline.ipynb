{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes made by kts4 / noami2\n",
    "- `glove` was not available and so needed to install `mittens` instead\n",
    "  - `from mittens import GloVe as glove`\n",
    "- Removed `merge` from import from `keras.layers`\n",
    "  - No longer available and not used.\n",
    "- `set_session`, `clear_session` and `get_session` are no longer available from `keras.backend.tensorflow_backend`\n",
    "  - Loaded from `tf.compat.v1.keras.backend` and `tf.keras.backend` instead\n",
    "- `reset_keras()` function was missing. \n",
    "  - Copied over from nb7\n",
    "- `tf.contrib.layers.l2_regularizer` no longer available\n",
    "  - Used `tf.keras.regularizers.l2` instead\n",
    "- `tf.contrib.layers.xavier_initializer` no longer available\n",
    "  - Used `tf.keras.initializers.GlorotUniform` instead (TODO: this is different to nb7!)\n",
    "- Needed to add function for `mean` so that the pickle files could be loaded\n",
    "  - Otherwise got error\n",
    "- We do not have the `FastText` model and so all references to it and combined model have to be commented out\n",
    "- `iter_num` was set to `2` rather than `11`\n",
    "  - Need it at `11` so that there are 10 runs of the code\n",
    "- Changes to newlines / spacings / printing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:21:27.449386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe as glove\n",
    "# from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "# from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras(model):\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect() # if it's done something you should see a number being outputted\n",
    "\n",
    "def create_dataset(dict_of_ner):\n",
    "    temp_data = []\n",
    "    for k, v in sorted(dict_of_ner.items()):\n",
    "        temp = []\n",
    "        for embed in v:\n",
    "            temp.append(embed)\n",
    "        temp_data.append(np.mean(temp, axis = 0)) \n",
    "    return np.asarray(temp_data)\n",
    "\n",
    "def make_prediction_multi_avg(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_multi_avg(predictions, probs, ground_truth,                           \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,                          \n",
    "                          sequence_name, type_of_ner):\n",
    "    \n",
    "    auc   = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "    \n",
    "    result_path = \"results/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def avg_ner_model(layer_name, number_of_unit, embedding_name):\n",
    "\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    else:\n",
    "        input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n",
    "#     x_1 = Dense(256, activation='relu')(input_avg)\n",
    "#     x_1 = Dropout(0.3)(x_1)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    x = keras.layers.Concatenate()([x, input_avg])\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    \n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.01)\n",
    "    \n",
    "    preds = Dense(1, \n",
    "                  activation='sigmoid',\n",
    "                  use_bias=False,\n",
    "                  kernel_initializer=tf.keras.initializers.GlorotUniform(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "                 )(x)\n",
    "    \n",
    "    \n",
    "    opt = Adam(lr=0.001, decay = 0.01)\n",
    "    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm   = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm  = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev   = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test  = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "# ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "# ner_concat   = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids   = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids  = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  GRU\n",
      "Hidden unit:  128\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:21:34.274203: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/gru/gru_cell/recurrent_kernel/v/Assign' id:689 op device:{requested: '', assigned: ''} def:{{{node training/Adam/gru/gru_cell/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/gru/gru_cell/recurrent_kernel/v, training/Adam/gru/gru_cell/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-06 12:21:38.246012: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:408 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-06 12:22:21.050225: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:342 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8854807433342311 0.5982690179895127 0.9190045248868778 0.4766081871345029\n",
      "WARNING:tensorflow:From /var/folders/yw/zfq33jrj5z762jl1nhmn5vvh0000gn/T/ipykernel_6663/3713362575.py:3: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "Layer:  GRU\n",
      "Hidden unit:  128\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:22:23.395381: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/decay/Assign' id:634 op device:{requested: '', assigned: ''} def:{{{node training/Adam/decay/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/decay, training/Adam/decay/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-06 12:22:26.870697: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:408 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-06 12:24:14.475914: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_1/Sigmoid' id:342 op device:{requested: '', assigned: ''} def:{{{node dense_1/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/MatMul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8944471020425686 0.5401198959720465 0.9438914027149321 0.4700854700854701\n",
      "Layer:  GRU\n",
      "Hidden unit:  128\n",
      "Embedding:  word2vec\n",
      "Iteration number:  1\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:24:18.272269: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/gru/gru_cell/bias/v/Assign' id:694 op device:{requested: '', assigned: ''} def:{{{node training/Adam/gru/gru_cell/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/gru/gru_cell/bias/v, training/Adam/gru/gru_cell/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-04-06 12:24:24.211565: W tensorflow/c/c_api.cc:300] Operation '{name:'loss/AddN' id:408 op device:{requested: '', assigned: ''} def:{{{node loss/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul, loss/dense_1/kernel/Regularizer/mul)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "embedding_types = ['word2vec']#, 'fasttext', 'concat']\n",
    "embedding_dict  = [ner_word2vec]#, ner_fasttext, ner_concat]\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size = 64\n",
    "iter_num = 11\n",
    "unit_sizes = [128, 256]\n",
    "\n",
    "#layers = [\"LSTM\", \"GRU\"]\n",
    "layers = [\"GRU\"]\n",
    "\n",
    "for each_layer in layers:\n",
    "    for each_unit_size in unit_sizes:\n",
    "        for embed_dict, embed_name in zip(embedding_dict, embedding_types): \n",
    "\n",
    "            temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n",
    "            temp_dev_ner   = dict((k, ner_word2vec[k]) for k in dev_ids)\n",
    "            temp_test_ner  = dict((k, ner_word2vec[k]) for k in test_ids)\n",
    "\n",
    "            x_train_ner = create_dataset(temp_train_ner)\n",
    "            x_dev_ner   = create_dataset(temp_dev_ner)\n",
    "            x_test_ner  = create_dataset(temp_test_ner)\n",
    "\n",
    "            for iteration in range(1, iter_num):\n",
    "                for each_problem in target_problems: \n",
    "                    \n",
    "                    print (\"Layer: \", each_layer) \n",
    "                    print (\"Hidden unit: \", each_unit_size) \n",
    "                    print (\"Embedding: \", embed_name)\n",
    "                    print (\"Iteration number: \", iteration)\n",
    "                    print (\"Problem type: \", each_problem)\n",
    "                    print (\"__________________\")\n",
    "\n",
    "                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, \n",
    "                                                           patience=model_patience)\n",
    "                    \n",
    "                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "                    \n",
    "                    checkpoint = ModelCheckpoint(best_model_name, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min', \n",
    "                                                 period=1)\n",
    "\n",
    "\n",
    "                    callbacks = [early_stopping_monitor, checkpoint]\n",
    "\n",
    "                    model = avg_ner_model(each_layer, \n",
    "                                          each_unit_size, \n",
    "                                          embed_name)\n",
    "                    \n",
    "                    model.fit([x_train_lstm, x_train_ner], \n",
    "                              y_train[each_problem], \n",
    "                              epochs=num_epoch, \n",
    "                              verbose=0, \n",
    "                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), \n",
    "                              callbacks=callbacks, \n",
    "                              batch_size=batch_size )\n",
    "\n",
    "                    model.load_weights(best_model_name)\n",
    "\n",
    "                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n",
    "                    \n",
    "                    save_scores_multi_avg(predictions, \n",
    "                                          probs, \n",
    "                                          y_test[each_problem], \n",
    "                                          embed_name, \n",
    "                                          each_problem, \n",
    "                                          iteration, \n",
    "                                          each_unit_size, \n",
    "                                          each_layer, \n",
    "                                          type_of_ner)\n",
    "                    \n",
    "                    reset_keras(model)\n",
    "                    #del model\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
